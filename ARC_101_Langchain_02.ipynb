{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40997752-7dbf-4971-8d17-f3dd557082fc",
   "metadata": {},
   "source": [
    "LangChain supports several completely free-to-use LLMs (Large Language Models) that you can use without any API costs. Here are some options:\n",
    "\n",
    "### **1. Local Open-Source LLMs (Run on Your Own Hardware)**\n",
    "These models are free to use but require you to download and run them locally (or on a free cloud instance like Google Colab). LangChain integrates with many via `HuggingFacePipeline` or `Ollama`.\n",
    "\n",
    "#### **Popular Free Models:**\n",
    "- **Mistral 7B / Mistral 7B Instruct** (Small but powerful)\n",
    "- **Llama 2 (7B, 13B, 70B)** (Metaâ€™s open-weight model, requires approval but free)\n",
    "- **Zephyr 7B** (Fine-tuned Mistral for chat)\n",
    "- **Gemma (2B/7B)** (Googleâ€™s lightweight open model)\n",
    "- **Phi-2 (2.7B)** (Microsoftâ€™s small but capable model)\n",
    "\n",
    "#### **How to Use Them in LangChain:**\n",
    "- Via **Ollama** (easy local setup):\n",
    "  ```python\n",
    "  from langchain_community.llms import Ollama\n",
    "  llm = Ollama(model=\"mistral\")  # or \"llama2\", \"zephyr\", etc.\n",
    "  ```\n",
    "- Via **HuggingFace Pipeline** (requires GPU):\n",
    "  ```python\n",
    "  from langchain_community.llms import HuggingFacePipeline\n",
    "  from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "  model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "  pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "  llm = HuggingFacePipeline(pipeline=pipe)\n",
    "  ```\n",
    "\n",
    "### **2. Free API-based LLMs (No Local Setup)**\n",
    "Some APIs offer limited free tiers, but truly free ones are rare. A few options:\n",
    "- **Ollama API** (if you host locally)\n",
    "- **HuggingFace Inference API** (free for small models)\n",
    "- **LocalAI** (self-hosted OpenAI-compatible API)\n",
    "\n",
    "### **Best Choice?**\n",
    "- If you have a decent GPU, run **Mistral 7B** or **Zephyr** locally via Ollama.\n",
    "- If you need a free API, try **HuggingFaceâ€™s free tier** for small models.\n",
    "\n",
    "Would you like help setting one up? ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbfe6976-e086-488e-a688-4a57288bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM,OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c703492d-179a-428d-943a-7fd47d711829",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cae6b5a-d993-4a66-9fbb-ca2e3e2b114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why don't we trust AI with our secrets? Because it keeps everything in the cloud! (Cloud storage joke)\n",
      "\n",
      "Or this one: Why did the AI go to therapy? Because it had too many issues with its neural network! (Therapy joke)\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "response = llm.invoke(\"Tell me a joke about AI.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a7d52d-087b-4aff-aef6-1a1d8a51bc76",
   "metadata": {},
   "source": [
    "# How to use a vectorstore as a retriever\n",
    "\n",
    "> - A vector store retriever is a retriever that uses a vector store to retrieve documents. It is a lightweight wrapper around the vector store class to make it conform to the retriever interface. It uses the search methods implemented by a vector store, like similarity search and MMR, to query the texts in the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d2e87-1819-44cb-aa53-04dcacbb39cd",
   "metadata": {},
   "source": [
    "## Creating a retriever from a vectorstore\n",
    "\n",
    "You can build a retriever from a vectorstore using its .as_retriever method. Let's walk through an example.\n",
    "\n",
    "First we instantiate a vectorstore. We will use an in-memory FAISS vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b4c275-8aa5-4cf1-81e8-566d2679399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaLLM,OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b188aa-3ad0-4ea7-ba2a-7c75feb68ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"state_of_the_union.txt\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c6009bb-d11f-4c68-b9c2-e18171c6a656",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e947367-9b52-4221-b159-c20c205affc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0724dcf7-14dd-4bab-b290-8a63f04671af",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e93177e-849f-44b8-aa51-ca4b5d53ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a27159-36b3-4884-99aa-be76ec88120b",
   "metadata": {},
   "source": [
    "- We can then instantiate a retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2ebe1fc-b032-436c-baa9-43068c8cad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498a56c-8db2-41c0-8ab8-e55a6502ca2e",
   "metadata": {},
   "source": [
    "- This creates a retriever (specifically a VectorStoreRetriever), which we can use in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c4f59fa-a102-4b9e-b988-99ad5041b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "786b3990-102d-4ea0-93d1-b9168dbbcb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0906fbd-3214-4955-b8b8-3b19e4241694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='0e5a1922-9689-4efb-9020-d85914f23ec2', metadata={'source': 'state_of_the_union.txt'}, page_content='One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebb430-9331-418d-9715-364c77506957",
   "metadata": {},
   "source": [
    "## Maximum marginal relevance retrieval\n",
    "By default, the vector store retriever uses similarity search. If the underlying vector store supports maximum marginal relevance search, you can specify that as the search type.\n",
    "\n",
    "This effectively specifies what method on the underlying vectorstore is used (e.g., `similarity_search`, `max_marginal_relevance_search`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8315adeb-ae4d-470b-a2d3-92225d3a6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b14247c-4227-4275-8ce7-f25f1e0d0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f81dbb72-457b-4648-937b-311af2f90f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='0e5a1922-9689-4efb-9020-d85914f23ec2', metadata={'source': 'state_of_the_union.txt'}, page_content='One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf98bca-45df-4e55-a8ee-4fba5c035197",
   "metadata": {},
   "source": [
    "## Passing search parameters\n",
    "\n",
    "We can pass parameters to the underlying vectorstore's search methods using `search_kwargs`.\n",
    "\n",
    "### Similarity score threshold retrieval\n",
    "\n",
    "For example, we can set a similarity score threshold and only return documents with a score above that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06f3eadd-bec3-498e-a400-e475192a8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3844fbf1-f7d5-4850-b765-cd9e7654bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b4191d-4a28-44b5-9678-9e61ba6c1f28",
   "metadata": {},
   "source": [
    "### Specifying top k\n",
    "\n",
    "We can also limit the number of documents `k` returned by the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fb244b1-4cb1-4979-a30b-cf4a2efacedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06b318ab-d051-4b31-9f91-6961cd549e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = retriever.invoke(\"what did the president say about ketanji brown jackson?\")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fccdf81-90f4-4ea7-9f26-43aaf7178785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='0e5a1922-9689-4efb-9020-d85914f23ec2', metadata={'source': 'state_of_the_union.txt'}, page_content='One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \\n\\nAnd I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddab3466-5a4d-49c9-80c4-d59541598e75",
   "metadata": {},
   "source": [
    "___________________________________________\n",
    "\n",
    "#### **Understanding the MultiQueryRetriever**  \n",
    "\n",
    "When searching a vector database (a system that stores data as numerical vectors), the system compares how \"close\" your search query is to stored documents using mathematical distance measurements. However, small changes in how you phrase your question can lead to very different results. Additionally, if the way the data is converted into vectors (embeddings) doesnâ€™t fully capture the meaning, the search might miss relevant information.  \n",
    "\n",
    "To fix this, people often tweak their search prompts manuallyâ€”but this can be time-consuming.  \n",
    "\n",
    "#### **How the MultiQueryRetriever Helps**  \n",
    "\n",
    "Instead of manually adjusting queries, the **MultiQueryRetriever** uses a large language model (LLM) to automatically generate **multiple versions** of your original question from different angles. For example:  \n",
    "- If you ask, *\"How does photosynthesis work?\"*, the retriever might also generate:  \n",
    "  - *\"Explain the process of photosynthesis in plants.\"*  \n",
    "  - *\"What are the steps involved in converting sunlight into energy in plants?\"*  \n",
    "\n",
    "It then searches the database for each variation and combines the results, ensuring a broader and more accurate set of documents.  \n",
    "\n",
    "#### **Why This Works Better**  \n",
    "- Reduces dependency on a single phrasing of the question.  \n",
    "- Captures related concepts that a basic search might miss.  \n",
    "- Saves time compared to manual prompt engineering.  \n",
    "\n",
    "#### **Example Setup**  \n",
    "Letâ€™s create a vector database using a blog post (like Lilian Wengâ€™s *\"LLM Powered Autonomous Agents\"*) and apply the MultiQueryRetriever to improve search results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6adec931-5cb9-4892-bb18-fa7744ae473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4491f472-1ceb-44c1-8e69-3cf6b105b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set USER_AGENT to avoid warnings\n",
    "os.environ[\"USER_AGENT\"] = \"LangChain-RAG-App/1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e470120-d0cc-4b65-9e64-a2c690d7391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    header_template={\"User-Agent\": os.environ[\"USER_AGENT\"]}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab381f21-b2cd-443f-a00f-f0d9a587b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a1f14398-9b8a-480e-98e7-5a2403755a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "39de4f8c-c244-4972-90eb-5316d86a0a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c680586-68cb-49a6-8758-416cc264630d",
   "metadata": {},
   "source": [
    "- Specify the LLM to use for query generation, and the retriever will do the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7302a54a-a503-4c56-b9f8-1ef568e7d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a730426-d466-4dc7-be82-d7a25cd6e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee68ef36-e949-4754-bd0f-2c5ca321f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1ff843c-0653-4d01-bd88-a39dcd0542f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. List the methods for breaking down complex tasks into smaller, manageable parts.', '2. Can you find documents detailing strategies for dividing a large task into multiple subtasks?', \"3. I'm looking for information on techniques used to decompose a complex problem into smaller, solvable pieces.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs = retriever_from_llm.invoke(question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92278a5-c50f-4303-a5c2-c17c6f6d8749",
   "metadata": {},
   "source": [
    "### **Customizing the MultiQueryRetriever's Prompt**  \n",
    "\n",
    "The **MultiQueryRetriever** automatically generates different versions of your search query to improve retrieval results. By default, it uses a predefined prompt, but you can customize it for better control.  \n",
    "\n",
    "#### **How Query Generation Works**  \n",
    "- The retriever uses an **LLM** to create multiple variations of your input question.  \n",
    "- These generated queries are logged at the **INFO** level (useful for debugging).  \n",
    "- Each query retrieves documents, and the final result combines all unique matches.  \n",
    "\n",
    "#### **Customizing the Prompt**  \n",
    "If you want to change how queries are generated:  \n",
    "\n",
    "1. **Create a Custom PromptTemplate**  \n",
    "   - Define a `PromptTemplate` with an input variable (e.g., `{question}`) for the original query.  \n",
    "   - Example:  \n",
    "     ```python\n",
    "     from langchain.prompts import PromptTemplate\n",
    "\n",
    "     custom_prompt = PromptTemplate(\n",
    "         input_variables=[\"question\"],\n",
    "         template=\"\"\"Generate 3 different versions of the following question to improve retrieval:\n",
    "         Original: {question}\n",
    "         Variations:\"\"\"\n",
    "     )\n",
    "     ```\n",
    "\n",
    "2. **Use an Output Parser**  \n",
    "   - The LLMâ€™s response must be split into a list of queries.  \n",
    "   - Example parser (splits by newlines or numbering):  \n",
    "     ```python\n",
    "     from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "     def parse_queries(output):\n",
    "         return [q.strip() for q in output.split(\"\\n\") if q.strip()]\n",
    "\n",
    "     # Or use a more advanced parser if needed\n",
    "     ```\n",
    "\n",
    "3. **Pass Both to the Retriever**  \n",
    "   - Configure the `MultiQueryRetriever` with your custom prompt and parser.  \n",
    "\n",
    "#### **Why Customize?**  \n",
    "- **Control query style** (e.g., force technical vs. simple language).  \n",
    "- **Adjust for domain-specific needs** (e.g., legal vs. casual searches).  \n",
    "- **Debugging** (logged queries help fine-tune prompts).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca57a2cd-fc09-4824-9694-b14bf3fbef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadfd571-eb9e-47f3-9242-df6cddc12c66",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **`BaseOutputParser`**: Base class for parsing structured output from LLMs.  \n",
    "- **`PromptTemplate`**: Used to define reusable prompt structures with variables.  \n",
    "- **`pydantic.BaseModel` & `Field`**: Used for data validation (though not directly used in this snippet).  \n",
    "\n",
    "---\n",
    "```python\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "```\n",
    "- **Purpose**: Defines a custom parser to split LLM output (text) into a list of strings.  \n",
    "- **Inherits from `BaseOutputParser`**: Ensures compatibility with LangChain's parsing system.  \n",
    "\n",
    "```python\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "```\n",
    "- **`text.strip()`**: Removes leading/trailing whitespace.  \n",
    "- **`split(\"\\n\")`**: Splits text by newlines into a list.  \n",
    "- **`filter(None, lines)`**: Removes empty strings (e.g., from extra newlines).  \n",
    "\n",
    "```python\n",
    "output_parser = LineListOutputParser()\n",
    "```\n",
    "- **Instantiates the parser** for later use in the chain.  \n",
    "\n",
    "---\n",
    "```python\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "```\n",
    "- **`input_variables=[\"question\"]`**: Defines `{question}` as the dynamic variable in the prompt.  \n",
    "- **`template`**: Instructs the LLM to generate 5 query variations, separated by newlines.  \n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "```\n",
    "- **`|` (Pipe Operator)**: Chains components together sequentially (LangChain's syntax).  \n",
    "- **Flow**:  \n",
    "  1. `QUERY_PROMPT` injects the user's `question` into the template.  \n",
    "  2. `llm` generates text (5 query variations).  \n",
    "  3. `output_parser` splits the LLM's response into a clean list of queries.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Key Notes**\n",
    "- **Output Parser**: Ensures the LLM's response is formatted as a list (e.g., for `MultiQueryRetriever`).  \n",
    "- **Prompt Engineering**: The template explicitly guides the LLM to generate diverse queries.  \n",
    "- **Temperature=0**: Used for consistency in query generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb979b00-2c2c-41c9-b21d-c65a0da6e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return list(filter(None, lines))  # Remove empty lines\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8a48df33-f80a-4ac6-842a-642a9e0889f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "llm_chain = QUERY_PROMPT | llm | output_parser\n",
    "\n",
    "# Other inputs\n",
    "question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a8ffe379-ee24-4998-b7dc-b5134412ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Can you find documents that explain the concepts covered in the course regarding regression?', \"2. I'm looking for information about the topic of regression as discussed in this course.\", '3. Could you provide me with materials that detail the aspects of regression taught in the given course?', \"4. What are the details about regression mentioned in the course I'm currently taking?\", \"5. I need documents that delve into the regression part of the course I'm enrolled in.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectordb.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\",# Ensure each sub-query fetches ranked results\n",
    "    search_kwargs={\"k\": 1}  # Gets top 5 per query\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.invoke(\"What does the course say about regression?\")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b357825-99c3-4cac-84f5-54439594ac8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dc3e0c2e-b4e3-4f2a-ac4d-0c17c72f6577', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'title': 'Large language model - Wikipedia'}, page_content='to the LLM planner can even be the LaTeX code of a paper describing the environment.[62]'),\n",
       " Document(id='440bec54-f4fc-4a11-8716-354f4eed53e5', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content=\"There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\"),\n",
       " Document(id='66c5b7f9-5f8e-43c5-8382-63a5052796d4', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Level-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.'),\n",
       " Document(id='1e7b856c-b938-4db3-b2db-2727e99a0bde', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.'),\n",
       " Document(id='89246b70-e2cc-473b-9ae5-74ab244ff2b0', metadata={'language': 'en', 'source': 'https://en.wikipedia.org/wiki/Large_language_model', 'title': 'Large language model - Wikipedia'}, page_content='Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with ChatGPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.'),\n",
       " Document(id='8da8ac97-c00b-4dd5-a17e-9ec7a8c4b201', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='inquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.'),\n",
       " Document(id='1e3b2bbe-6e2c-4170-844b-1e94997529f6', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='to API search engine to find the right API to call and then uses the corresponding documentation to make a call.'),\n",
       " Document(id='0575ed8b-a199-4448-af2d-3035a9db545f', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='to start a new trial depending on the self-reflection results.'),\n",
       " Document(id='189ea47c-1453-42b4-9eb2-fdff5d25e7b0', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.'),\n",
       " Document(id='37d29b9e-2a8c-459e-b9af-7dc893898c2a', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='The LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.'),\n",
       " Document(id='1a8981b8-0000-4017-8be7-4d36cc9ae8dd', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agentâ€™s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='related to KD tree but a lot more scalable.')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fb5d9-72d1-4d3d-85da-89aa97422adc",
   "metadata": {},
   "source": [
    "# **Using Pydantic for Structured, Validated LLM Outputs**  \n",
    "\n",
    "When working with LLMs, we often want structured outputs (e.g., JSON) instead of raw text. **Pydantic** makes this easy by enforcing type safety and validation.  \n",
    "\n",
    "#### **How It Works**  \n",
    "1. **Define a Pydantic Model**  \n",
    "   - Create a class specifying the expected fields and types.  \n",
    "   - Example:  \n",
    "     ```python\n",
    "     from pydantic import BaseModel\n",
    "\n",
    "     class UserProfile(BaseModel):\n",
    "         name: str\n",
    "         age: int\n",
    "         email: str | None = None  # Optional field\n",
    "     ```\n",
    "\n",
    "2. **Pass the Model to the LLM**  \n",
    "   - LangChain (or OpenAI) will force the LLM to generate output matching the schema.  \n",
    "     ```\n",
    "\n",
    "3. **Automatic Validation**  \n",
    "   - Pydantic checks:  \n",
    "     - Required fields (e.g., `name` canâ€™t be missing).  \n",
    "     - Data types (e.g., `age` must be an `int`).  \n",
    "   - Raises `ValidationError` if the LLMâ€™s output doesnâ€™t match.  \n",
    "\n",
    "#### **Key Benefits**  \n",
    "âœ… **Structured Outputs** â€“ No more parsing messy text.  \n",
    "âœ… **Validation** â€“ Catches LLM hallucinations early.  \n",
    "âœ… **Integration** â€“ Works with LangChain, OpenAI, and others.  \n",
    "\n",
    "#### **Use Cases**  \n",
    "- Extracting entities (e.g., dates, names).  \n",
    "- APIs requiring strict input formats.  \n",
    "- Building RAG systems with validated responses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a752d392-c6ff-4138-af7d-c7c8521ddb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional,List\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e092a134-8d3c-44e2-907f-5416b636f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customResearch(BaseModel):\n",
    "    title: str = Field(description=\"The title of the query\")\n",
    "    summary: str = Field(description=\"The summary of the answer to the query\")\n",
    "    keyword: List[str] = Field(\n",
    "         description=\"The keyword in the raw response to the query give me atleast 10\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "fd71592c-363a-442d-af40-0dc591942768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a prompt that forces JSON output\n",
    "template = \"\"\"Return a JSON object matching this schema:\n",
    "{{\n",
    "    \"title\": \"string\",\n",
    "    \"keyword\": [\"string\"],\n",
    "    \"summary\": \"string\"\n",
    "}}\n",
    "\n",
    "Generate a research topic about {input} give me atleast 10 keywords and a long summary. Output ONLY valid JSON:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "93b38608-d04d-4680-bff1-4a27f97b57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set up the parser\n",
    "parser = PydanticOutputParser(pydantic_object=customResearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cf2b7556-2ebd-4884-b7d6-9da7ff70bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Chain everything together\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a405b38f-5d6e-4839-aaa3-bca1187ecd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Exploring the Ethical Implications of Artificial Intelligence' summary='This research delves into the complex ethical landscape of Artificial Intelligence (AI). It examines the philosophical underpinnings of ethics, focusing on how these principles apply to AI autonomous decision-making. The study also investigates the impact of bias and fairness in AI systems and the need for diversity in data sets. Privacy concerns, transparency, accountability, and safety are critical areas addressed within this research. Furthermore, it considers the potential for effective human-AI collaboration and proposes regulatory frameworks to ensure ethical use of AI.' keyword=['Artificial Intelligence', 'Ethics', 'Moral Philosophy', 'Autonomous Decision-making', 'Bias and Fairness', 'Privacy', 'Transparency', 'Accountability', 'Safety and Security', 'Human-AI Collaboration', 'Regulation']\n"
     ]
    }
   ],
   "source": [
    "# Run it\n",
    "result = chain.invoke({\"input\": \"AI ethics\"})\n",
    "print(result)\n",
    "# Output: ResearchTopic(title=\"...\", keywords=[\"...\"], summary=\"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (arash)",
   "language": "python",
   "name": "arash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
